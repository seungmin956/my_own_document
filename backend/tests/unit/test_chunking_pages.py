"""
ì²­í‚¹ ì „í›„ í˜ì´ì§€ ì •ë³´ ì¶”ì 

ë¬¸ì œ: ì²­í‚¹ í›„ ëª¨ë“  ì²­í¬ê°€ page=1
ì›ì¸: í™•ì¸ í•„ìš”
"""

from backend.preprocessor.pdf_loader import load_pdf
from langchain_text_splitters import RecursiveCharacterTextSplitter


def test_chunking_page_preservation():
    """ì²­í‚¹ ì‹œ í˜ì´ì§€ ì •ë³´ ë³´ì¡´ í™•ì¸"""

    file_path = "./data/_10-K-Q4-2023-As-Filed.pdf"

    print("=" * 70)
    print("ğŸ” ì²­í‚¹ ì „í›„ í˜ì´ì§€ ì •ë³´ ì¶”ì ")
    print("=" * 70)

    # 1. PDF ë¡œë“œ
    print("\n[1] PDF ë¡œë”©...")
    docs, loader_name = load_pdf(file_path)

    print(f"   ë¡œë”: {loader_name}")
    print(f"   ë¬¸ì„œ ìˆ˜: {len(docs)}ê°œ")

    # ì²˜ìŒ 10ê°œ ë¬¸ì„œì˜ í˜ì´ì§€ í™•ì¸
    print(f"\nì²˜ìŒ 10ê°œ ë¬¸ì„œì˜ í˜ì´ì§€ ì •ë³´:")
    print(f"{'â”€'*70}")

    for i, doc in enumerate(docs[:10], 1):
        page = doc.metadata.get("page", "NONE")
        text_len = len(doc.page_content)
        text_sample = doc.page_content[:50].replace("\n", " ")

        print(f"[{i}] page={page} | {text_len}ì | {text_sample}...")

    # 2. ì²­í‚¹ í…ŒìŠ¤íŠ¸ (í˜„ì¬ ë°©ì‹)
    print(f"\n{'â”€'*70}")
    print("[2] ì²­í‚¹ í…ŒìŠ¤íŠ¸ - ì „ì²´ ë¬¸ì„œ í•œë²ˆì—")
    print(f"{'â”€'*70}")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )

    chunks_bad = splitter.split_documents(docs)

    print(f"   ì²­í¬ ìˆ˜: {len(chunks_bad)}ê°œ")
    print(f"\nì²˜ìŒ 10ê°œ ì²­í¬ì˜ í˜ì´ì§€ ì •ë³´:")

    for i, chunk in enumerate(chunks_bad[:10], 1):
        page = chunk.metadata.get("page", "NONE")
        text_len = len(chunk.page_content)
        text_sample = chunk.page_content[:50].replace("\n", " ")

        print(f"[{i}] page={page} | {text_len}ì | {text_sample}...")

    # í˜ì´ì§€ ë¶„í¬
    page_counts_bad = {}
    for chunk in chunks_bad:
        page = chunk.metadata.get("page", -1)
        page_counts_bad[page] = page_counts_bad.get(page, 0) + 1

    print(f"\ní˜ì´ì§€ ë¶„í¬:")
    for page in sorted(page_counts_bad.keys())[:10]:
        print(f"   page {page}: {page_counts_bad[page]}ê°œ")

    # 3. ì²­í‚¹ í…ŒìŠ¤íŠ¸ (ê°œë³„ ë¬¸ì„œ)
    print(f"\n{'â”€'*70}")
    print("[3] ì²­í‚¹ í…ŒìŠ¤íŠ¸ - ê° ë¬¸ì„œ ê°œë³„ ì²­í‚¹")
    print(f"{'â”€'*70}")

    chunks_good = []

    for doc in docs:
        doc_chunks = splitter.split_documents([doc])

        # í˜ì´ì§€ ì •ë³´ ë³´ì¡´
        original_page = doc.metadata.get("page", 0)
        for chunk in doc_chunks:
            chunk.metadata["page"] = original_page

        chunks_good.extend(doc_chunks)

    print(f"   ì²­í¬ ìˆ˜: {len(chunks_good)}ê°œ")
    print(f"\nì²˜ìŒ 10ê°œ ì²­í¬ì˜ í˜ì´ì§€ ì •ë³´:")

    for i, chunk in enumerate(chunks_good[:10], 1):
        page = chunk.metadata.get("page", "NONE")
        text_len = len(chunk.page_content)
        text_sample = chunk.page_content[:50].replace("\n", " ")

        print(f"[{i}] page={page} | {text_len}ì | {text_sample}...")

    # í˜ì´ì§€ ë¶„í¬
    page_counts_good = {}
    for chunk in chunks_good:
        page = chunk.metadata.get("page", -1)
        page_counts_good[page] = page_counts_good.get(page, 0) + 1

    print(f"\ní˜ì´ì§€ ë¶„í¬ (ì²˜ìŒ 20í˜ì´ì§€):")
    for page in sorted(page_counts_good.keys())[:20]:
        print(f"   page {page}: {page_counts_good[page]}ê°œ")

    # 4. ë¹„êµ
    print(f"\n{'='*70}")
    print("ê²°ê³¼ ë¹„êµ")
    print(f"{'='*70}")

    print(f"\n[ì „ì²´ ì²­í‚¹] í˜ì´ì§€ ì¢…ë¥˜: {len(page_counts_bad)}ê°œ")
    print(f"   {list(sorted(page_counts_bad.keys())[:10])}")

    print(f"\n[ê°œë³„ ì²­í‚¹] í˜ì´ì§€ ì¢…ë¥˜: {len(page_counts_good)}ê°œ")
    print(f"   {list(sorted(page_counts_good.keys())[:10])}")

    if len(page_counts_good) > len(page_counts_bad):
        print(f"\nâœ… ê°œë³„ ì²­í‚¹ì´ í˜ì´ì§€ ì •ë³´ë¥¼ ë” ì˜ ë³´ì¡´í•©ë‹ˆë‹¤!")
    else:
        print(f"\nâš ï¸  ì—¬ì „íˆ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    test_chunking_page_preservation()
