# embedding_generator.py

from typing import List, Optional
from enum import Enum
import os
from pathlib import Path


class EmbeddingMode(Enum):
    """ì„ë² ë”© ëª¨ë“œ"""

    LOCAL = "local"  # Ollama (bge-m3, nomic-embed-text ë“±)
    OPENAI = "openai"  # OpenAI API


class EmbeddingGenerator:
    """
    í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±ê¸°

    ì§€ì› ëª¨ë“œ:
    - LOCAL: Ollama ë¡œì»¬ ëª¨ë¸ (bge-m3, nomic-embed-text)
    - OPENAI: OpenAI API (text-embedding-3-small)

    ì‚¬ìš© ì˜ˆì‹œ:
    >>> generator = EmbeddingGenerator(mode="local", model="bge-m3")
    >>> embeddings = generator.embed_documents(["í…ìŠ¤íŠ¸1", "í…ìŠ¤íŠ¸2"])
    >>> len(embeddings[0])  # ì°¨ì› í™•ì¸
    1024
    """

    # ì§€ì›í•˜ëŠ” ë¡œì»¬ ëª¨ë¸ë“¤
    SUPPORTED_LOCAL_MODELS = {
        "bge-m3": {
            "dimension": 1024,
            "max_tokens": 8192,
            "description": "ë‹¤êµ­ì–´ ìµœê°•, í•œê¸€ ìš°ìˆ˜, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰",
        },
        "nomic-embed-text": {
            "dimension": 768,
            "max_tokens": 8192,
            "description": "ë¹ ë¥´ê³  ê°€ë²¼ì›€, ë²”ìš©",
        },
        "mxbai-embed-large": {
            "dimension": 1024,
            "max_tokens": 512,
            "description": "ì˜ì–´ íŠ¹í™”, ì§§ì€ ì»¨í…ìŠ¤íŠ¸",
        },
    }

    def __init__(
        self,
        mode: str = None,
        model: str = None,
        batch_size: int = 32,
        verbose: bool = True,
    ):
        """
        Args:
            mode: 'local' ë˜ëŠ” 'openai' (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ EMBEDDING_MODE ì‚¬ìš©)
            model: ëª¨ë¸ ì´ë¦„ (LOCAL: bge-m3/nomic-embed-text, OPENAI: text-embedding-3-small)
            batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (GPU ë©”ëª¨ë¦¬ ì‘ìœ¼ë©´ ì¤„ì´ê¸°)
            verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€
        """
        # ëª¨ë“œ ê²°ì • (í™˜ê²½ë³€ìˆ˜ â†’ íŒŒë¼ë¯¸í„° â†’ ê¸°ë³¸ê°’)
        self.mode = mode or os.getenv("EMBEDDING_MODE", "local")
        self.batch_size = batch_size
        self.verbose = verbose

        # ëª¨ë¸ ì´ˆê¸°í™”
        if self.mode == "local":
            self.model_name = model or os.getenv("EMBEDDING_MODEL", "bge-m3")
            self._init_local_embeddings()
        elif self.mode == "openai":
            self.model_name = model or "text-embedding-3-small"
            self._init_openai_embeddings()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {self.mode}")

        if self.verbose:
            print(f"âœ“ ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”: {self.mode} ëª¨ë“œ, ëª¨ë¸={self.model_name}")

    def _init_local_embeddings(self):
        """Ollama ë¡œì»¬ ì„ë² ë”© ì´ˆê¸°í™”"""
        from langchain_ollama import OllamaEmbeddings

        if self.model_name not in self.SUPPORTED_LOCAL_MODELS:
            print(f"âš ï¸  '{self.model_name}'ëŠ” í…ŒìŠ¤íŠ¸ë˜ì§€ ì•Šì€ ëª¨ë¸ì…ë‹ˆë‹¤.")

        # GPU ìë™ ê°ì§€
        num_gpu = self._detect_gpu()

        ollama_kwargs = {"model": self.model_name}
        if num_gpu > 0:
            ollama_kwargs["num_gpu"] = num_gpu
            if self.verbose:
                print(f"  âœ“ GPU ê°ì§€ë¨: {num_gpu}ê°œ GPU ì‚¬ìš©")
        else:
            if self.verbose:
                print(f"  âš ï¸  GPU ë¯¸ê°ì§€: CPU ëª¨ë“œë¡œ ì‘ë™ (ëŠë¦´ ìˆ˜ ìˆìŒ)")

        self.embeddings = OllamaEmbeddings(**ollama_kwargs)

        # ëª¨ë¸ ì •ë³´
        model_info = self.SUPPORTED_LOCAL_MODELS.get(self.model_name, {})
        self.dimension = model_info.get("dimension", "unknown")
        self.max_tokens = model_info.get("max_tokens", "unknown")

        if self.verbose and model_info:
            print(f"  - ì°¨ì›: {self.dimension}")
            print(f"  - ìµœëŒ€ í† í°: {self.max_tokens}")

    def _detect_gpu(self) -> int:
        """
        GPU ìë™ ê°ì§€
        Returns:
            int: ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜ (0ì´ë©´ CPUë§Œ ì‚¬ìš©)
        """
        try:
            import torch
            if torch.cuda.is_available():
                num_gpus = torch.cuda.device_count()
                return num_gpus
        except ImportError:
            pass

        # torch ì—†ê±°ë‚˜ CUDA ì—†ìœ¼ë©´ CPU
        return 0

    def _init_openai_embeddings(self):
        """OpenAI ì„ë² ë”© ì´ˆê¸°í™”"""
        from langchain_openai import OpenAIEmbeddings

        # API í‚¤ í™•ì¸
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError(
                "OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                ".env íŒŒì¼ì— OPENAI_API_KEY=sk-... ì¶”ê°€í•˜ì„¸ìš”."
            )

        self.embeddings = OpenAIEmbeddings(
            model=self.model_name, openai_api_key=api_key
        )

        # ëª¨ë¸ë³„ ì°¨ì›
        dimensions = {
            "text-embedding-3-small": 1536,
            "text-embedding-3-large": 3072,
            "text-embedding-ada-002": 1536,
        }
        self.dimension = dimensions.get(self.model_name, 1536)
        self.max_tokens = 8191

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸ [[0.1, 0.2, ...], ...]
        """
        if not texts:
            return []

        total = len(texts)
        all_embeddings = []

        if self.verbose:
            print(f"\nğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘... (ì´ {total}ê°œ)")

        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
        for i in range(0, total, self.batch_size):
            batch = texts[i : i + self.batch_size]

            try:
                batch_embeddings = self.embeddings.embed_documents(batch)
                all_embeddings.extend(batch_embeddings)

                if self.verbose and (i + self.batch_size) % 100 == 0:
                    progress = min(i + self.batch_size, total)
                    print(f"  ì§„í–‰: {progress}/{total} ({progress/total*100:.1f}%)")

            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {i}-{i+len(batch)} ì„ë² ë”© ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” 0 ë²¡í„°ë¡œ ì±„ìš°ê¸°
                all_embeddings.extend([[0.0] * self.dimension] * len(batch))

        if self.verbose:
            print(f"âœ“ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(all_embeddings)}ê°œ\n")

        return all_embeddings

    def embed_query(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            text: ì„ë² ë”©í•  ì¿¼ë¦¬ í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° [0.1, 0.2, ...]
        """
        return self.embeddings.embed_query(text)

    def get_info(self) -> dict:
        """ì„ë² ë”© ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "mode": self.mode,
            "model": self.model_name,
            "dimension": self.dimension,
            "max_tokens": self.max_tokens,
            "batch_size": self.batch_size,
        }


# í¸ì˜ í•¨ìˆ˜
def create_embeddings(
    texts: List[str], mode: str = "local", model: str = "bge-m3", verbose: bool = True
) -> List[List[float]]:
    """
    ê°„ë‹¨í•œ ì„ë² ë”© ìƒì„± í•¨ìˆ˜

    Args:
        texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        mode: 'local' ë˜ëŠ” 'openai'
        model: ëª¨ë¸ ì´ë¦„
        verbose: ì§„í–‰ ìƒí™© ì¶œë ¥

    Returns:
        ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
    """
    generator = EmbeddingGenerator(mode=mode, model=model, verbose=verbose)
    return generator.embed_documents(texts)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("=" * 70)
    print("ğŸ§ª ì„ë² ë”© ìƒì„±ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_texts = [
        "ì¸ê³µì§€ëŠ¥ì€ í˜„ëŒ€ ì‚¬íšŒë¥¼ ë³€í™”ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤.",
        "Machine learning is a subset of artificial intelligence.",
        "ë”¥ëŸ¬ë‹ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‘ìš©ì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.",
    ]

    # LOCAL ëª¨ë“œ í…ŒìŠ¤íŠ¸
    print("\n[LOCAL ëª¨ë“œ - bge-m3]")
    generator = EmbeddingGenerator(mode="local", model="bge-m3")
    embeddings = generator.embed_documents(test_texts)

    print(f"ìƒì„±ëœ ì„ë² ë”© ê°œìˆ˜: {len(embeddings)}")
    print(f"ì„ë² ë”© ì°¨ì›: {len(embeddings[0])}")
    print(f"ì²« ë²ˆì§¸ ë²¡í„° ìƒ˜í”Œ: {embeddings[0][:5]}...")

    # ë‹¨ì¼ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
    print("\n[ì¿¼ë¦¬ ì„ë² ë”© í…ŒìŠ¤íŠ¸]")
    query = "AI ê¸°ìˆ ì´ë€ ë¬´ì—‡ì¸ê°€?"
    query_embedding = generator.embed_query(query)
    print(f"ì¿¼ë¦¬ ì„ë² ë”© ì°¨ì›: {len(query_embedding)}")

    # ëª¨ë¸ ì •ë³´
    print("\n[ëª¨ë¸ ì •ë³´]")
    info = generator.get_info()
    for key, value in info.items():
        print(f"  {key}: {value}")

    print("\n" + "=" * 70)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
