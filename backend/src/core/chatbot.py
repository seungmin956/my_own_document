# chatbot.py
"""
RAG ê¸°ë°˜ ë¬¸ì„œ ì±—ë´‡ (Reranking í†µí•©)

ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸:
1. Vector Search (ë¹ ë¥´ê²Œ 10ê°œ í›„ë³´)
2. Cross-Encoder Reranking (ì •í™•í•˜ê²Œ 5ê°œ ì„ íƒ)
3. LLM ë‹µë³€ ìƒì„±
"""

import os
from pathlib import Path
from typing import List, Dict, Optional
from dotenv import load_dotenv
from langchain_ollama import ChatOllama
from langsmith import traceable
from src.core.exceptions import (
    QdrantConnectionError,
    OllamaConnectionError,
    DocumentNotFoundError,
    SearchError,
    LLMGenerationError,
)
from src.services.retrieval.bm25_retriever import BM25Retriever, hybrid_search

# .env íŒŒì¼ ëª…ì‹œì  ë¡œë”© (ìˆ˜ì •ë¨!) â­
env_path = Path(__file__).parent.parent.parent / ".env"
load_dotenv(env_path)

# í™˜ê²½ ê°ì§€
APP_ENV = os.getenv("APP_ENV", "production")
IS_PRODUCTION = APP_ENV == "production"

# ë””ë²„ê·¸: í™˜ê²½ ë³€ìˆ˜ í™•ì¸
if os.getenv("DEBUG", "false").lower() == "true":
    print(f"\n[DEBUG] í™˜ê²½ ë³€ìˆ˜:")
    print(f"  .env ê²½ë¡œ: {env_path}")
    print(f"  .env ì¡´ì¬: {env_path.exists()}")
    print(f"  APP_ENV: {APP_ENV}")
    print(f"  IS_PRODUCTION: {IS_PRODUCTION}")
    print()

if IS_PRODUCTION:
    from src.core.user_config import UserConfig as Config
else:
    from src.core import config as dev_config

    class Config:
        """ê°œë°œìš© ì„¤ì • ë˜í¼"""

        def __init__(self):
            self.qdrant_host = dev_config.QDRANT_HOST
            self.qdrant_port = dev_config.QDRANT_PORT
            self.qdrant_api_key = dev_config.QDRANT_API_KEY
            # Model Configuration (from .env)
            self.embedding_model = dev_config.EMBEDDING_MODEL
            self.llm_model = dev_config.LLM_MODEL
            self.llm_temperature = dev_config.LLM_TEMPERATURE
            self.top_k = 5
            self.score_threshold = 0.4
            # Reranking ì„¤ì •
            self.rerank_enabled = True
            self.rerank_model = dev_config.RERANK_MODEL
            self.rerank_max_candidates = 10
            self.rerank_top_k = 3
            # BM25 ì„¤ì • ì¶”ê°€ â­
            self.bm25_enabled = dev_config.BM25_ENABLED
            self.bm25_vector_weight = dev_config.BM25_VECTOR_WEIGHT
            self.bm25_weight = dev_config.BM25_BM25_WEIGHT


from src.services.storage.qdrant_manager import QdrantManager
from src.services.retrieval.embedding_generator import EmbeddingGenerator
from src.services.retrieval.reranker import OptimizedReranker as Reranker

# LangSmith ì„¤ì •
LANGCHAIN_TRACING_V2 = os.getenv("LANGCHAIN_TRACING_V2", "false").lower() == "true"
LANGCHAIN_PROJECT = os.getenv("LANGCHAIN_PROJECT", "document-assistant-rag")


class DocumentChatbot:
    """
    RAG ê¸°ë°˜ ë¬¸ì„œ QA ì±—ë´‡ (Reranking í†µí•©)

    ê²€ìƒ‰ ì „ëµ:
    - Reranking OFF: Vector Searchë§Œ (ë¹ ë¦„)
    - Reranking ON: Vector + Cross-Encoder (ì •í™•)
    """

    def __init__(self, config: Optional[Config] = None):
        """
        Args:
            config: ì„¤ì • ê°ì²´ (Noneì´ë©´ ìë™ ìƒì„±)
        """
        # ì„¤ì • ë¡œë“œ
        if config is None:
            config = Config()

        self.config = config

        # 1. Qdrant ë§¤ë‹ˆì € (ì—°ê²° í™•ì¸ í¬í•¨)
        try:
            self.qdrant = QdrantManager(
                host=config.qdrant_host,
                port=config.qdrant_port,
                api_key=config.qdrant_api_key,
                embedding_dimension=1024,
                verbose=False,
            )
        except ConnectionError as e:
            # QdrantManager ë‚´ë¶€ì—ì„œ ì´ë¯¸ ConnectionError ë°œìƒ
            raise QdrantConnectionError(
                host=config.qdrant_host, port=config.qdrant_port
            )

        # 2. ì„ë² ë”© ìƒì„±ê¸° (ë¨¼ì € ìƒì„±)
        self.embedding_generator = EmbeddingGenerator(
            mode="local", model=config.embedding_model, verbose=False
        )

        # 3. Ollama ì—°ê²° í™•ì¸ (ìƒì„± í›„ í™•ì¸)
        try:
            self.embedding_generator.embed_query("test")
        except Exception as e:
            raise OllamaConnectionError(model=config.embedding_model)

        # 4. LLM ì´ˆê¸°í™”
        self.llm = ChatOllama(
            model=config.llm_model,
            temperature=config.llm_temperature,
            streaming=True,
        )

        # 5. Reranker (ì§€ì—° ë¡œë”©)
        self._reranker = None
        self.rerank_enabled = config.rerank_enabled

        if self.rerank_enabled:
            self._init_reranker(model_size=config.rerank_model, verbose=True)

        # ê²€ìƒ‰ ì„¤ì •
        self.score_threshold = config.score_threshold

        # Reranking ì„¤ì •
        if self.rerank_enabled:
            self.vector_top_k = config.rerank_max_candidates
            self.final_top_k = config.rerank_top_k
        else:
            self.vector_top_k = config.top_k
            self.final_top_k = config.top_k

        # 6. BM25 Retriever (ì„ íƒì )
        self.bm25_enabled = getattr(config, "bm25_enabled", False)
        if self.bm25_enabled:
            self.bm25_retriever = BM25Retriever(
                qdrant_manager=self.qdrant, verbose=False
            )
            self.bm25_vector_weight = getattr(config, "bm25_vector_weight", 0.7)
            self.bm25_weight = getattr(config, "bm25_weight", 0.3)
        else:
            self.bm25_retriever = None

        # ì´ˆê¸°í™” ë©”ì‹œì§€
        env_type = "í”„ë¡œë•ì…˜" if IS_PRODUCTION else "ê°œë°œ"

        print(f"\n{'='*70}")
        print(f"[INIT] Document Chatbot ì´ˆê¸°í™” ({env_type})")
        print(f"{'='*70}")

        if IS_PRODUCTION:
            print(f"  ì„¤ì •: ì‚¬ìš©ì ì„¤ì • (~/.document-assistant)")
        else:
            print(f"  ì„¤ì •: ê°œë°œ ì„¤ì • (.env)")

        print(f"  Qdrant: {config.qdrant_host}:{config.qdrant_port}")
        print(f"  Embedding: {config.embedding_model}")
        print(f"  LLM: {config.llm_model}")
        print(f"  ê²€ìƒ‰ ì „ëµ:")

        if self.bm25_enabled:
            print(f"    1. Hybrid Search (Vector + BM25)")
            print(
                f"       - Weights: Vector {self.bm25_vector_weight:.1f} / BM25 {self.bm25_weight:.1f}"
            )
            if self.rerank_enabled:
                print(f"    2. Reranking: {config.rerank_model} â†’ {self.final_top_k}ê°œ")
            print(f"       - Candidates: {self.vector_top_k}")
        elif self.rerank_enabled:
            print(
                f"    1. Vector Search: top_k={self.vector_top_k}, threshold={self.score_threshold}"
            )
            print(f"    2. Reranking: {config.rerank_model} â†’ top_k={self.final_top_k}")
        else:
            print(
                f"    Vector Searchë§Œ: top_k={self.vector_top_k}, threshold={self.score_threshold}"
            )

        if LANGCHAIN_TRACING_V2:
            print(f"  LangSmith: Enabled")

        print(f"{'='*70}\n")

    def _init_reranker(self, model_size: str = "auto", verbose: bool = False):
        """Reranker ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)"""
        self._reranker = Reranker(
            model_size=model_size, max_length=256, verbose=verbose
        )

    @traceable(name="rag_ask", tags=["rag", "chatbot"])
    def ask(
        self,
        question: str,
        doc_name: Optional[str] = None,
        verbose: bool = True,
    ) -> Dict:
        try:
            if verbose:
                print(f"\n{'='*70}")
                print(f"[QUESTION] {question}")
                print(f"{'='*70}\n")

            # 1. ì„ë² ë”©
            if verbose:
                print("[1/4] ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì¤‘...")
            query_embedding = self._embed_query(question)

            # 2. Vector Search (ë˜ëŠ” Hybrid Search)
            if verbose:
                search_type = (
                    "Hybrid Search (Vector + BM25)"
                    if self.bm25_enabled
                    else "Vector Search"
                )
                print(f"[2/4] {search_type} ì¤‘ (top_k={self.vector_top_k})...")

            search_results = self._search(
                query_embedding, question, doc_name
            )  # query ì¸ì ì¶”ê°€!

            if not search_results:
                if verbose:
                    print("\n[WARNING] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
                return {
                    "question": question,
                    "answer": "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "sources": [],
                    "error": None,
                }

            if verbose:
                print(f"   ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ì²­í¬")
                for i, result in enumerate(search_results[:3], 1):
                    toc_section = result["metadata"].get("toc_section", "Unknown")
                    print(
                        f"   [{i}] ì ìˆ˜: {result['score']:.3f} | "
                        f"{toc_section} (p.{result['metadata']['page']})"
                    )
                if len(search_results) > 3:
                    print(f"   ... ì™¸ {len(search_results) - 3}ê°œ")

            # 3. Reranking
            if self.rerank_enabled and self._reranker:
                if verbose:
                    print(
                        f"\n[3/4] Reranking ì¤‘ ({len(search_results)}ê°œ â†’ {self.final_top_k}ê°œ)..."
                    )

                try:
                    reranked_results = self._reranker.rerank(
                        query=question,
                        candidates=search_results,
                        top_k=self.final_top_k,
                    )

                    if verbose:
                        print(f"   [OK] Reranking ì™„ë£Œ: {len(reranked_results)}ê°œ ì„ íƒ")
                        for i, result in enumerate(reranked_results[:3], 1):
                            toc_section = result["metadata"].get(
                                "toc_section", "Unknown"
                            )
                            print(
                                f"   [{i}] Rerank: {result['rerank_score']:.3f} | "
                                f"Vector: {result['score']:.3f} | "
                                f"{toc_section}"
                            )

                    final_results = reranked_results

                except Exception as e:
                    if verbose:
                        print(f"\n[WARNING] Reranking ì‹¤íŒ¨: {e}")
                        print(f"   Vector Search ê²°ê³¼ ì‚¬ìš©\n")
                    final_results = search_results[: self.final_top_k]
            else:
                if verbose:
                    print(f"\n[3/4] Reranking ê±´ë„ˆëœ€")
                final_results = search_results[: self.final_top_k]

            # 4. ë‹µë³€ ìƒì„±
            if verbose:
                print(f"\n[4/4] ë‹µë³€ ìƒì„± ì¤‘ (LLM: {self.llm.model})...")

            answer = self._generate_answer(question, final_results, verbose=verbose)

            if verbose:
                print(f"\n{'='*70}")
                print("[ANSWER]")
                print(f"{'='*70}")
                print(answer)
                print(f"\n{'='*70}")
                print("[SOURCES]")
                print(f"{'='*70}")

                sources = self._extract_sources(final_results)
                for i, source in enumerate(sources, 1):
                    toc_section = source.get("toc_section", "Unknown")
                    score_info = (
                        f"Rerank: {source.get('rerank_score', 0):.3f}"
                        if "rerank_score" in source
                        else f"Vector: {source['score']:.3f}"
                    )
                    print(
                        f"[{i}] {toc_section} (p.{source['page']}) | "
                        f"ë¬¸ì„œ: {source['doc_name']} | {score_info}"
                    )
                print(f"{'='*70}\n")

            return {
                "question": question,
                "answer": answer,
                "sources": self._extract_sources(final_results),
                "search_results": final_results,
                "error": None,
            }

        except (
            QdrantConnectionError,
            OllamaConnectionError,
            DocumentNotFoundError,
        ) as e:
            return {
                "question": question,
                "answer": None,
                "sources": [],
                "error": e.to_dict(),
            }

        except Exception as e:
            return {
                "question": question,
                "answer": None,
                "sources": [],
                "error": {
                    "error": "UnexpectedError",
                    "message": f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}",
                    "solution": "ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”",
                },
            }

    @traceable(name="embed_query", tags=["embedding"])
    def _embed_query(self, question: str) -> List[float]:
        return self.embedding_generator.embed_query(question)

    @traceable(name="vector_search", tags=["retrieval"])
    def _search(
        self, query_vector: List[float], query: str, doc_name: Optional[str] = None
    ) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ ë˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        try:
            if doc_name:
                collections = self.qdrant.list_collections(doc_name)
                if not collections:
                    return []
            else:
                collections = None

            # ë²¡í„° ê²€ìƒ‰
            vector_results = self.qdrant.search(
                query_vector=query_vector,
                collection_names=collections,
                limit=self.vector_top_k,
                score_threshold=self.score_threshold,
            )
            # BM25 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            if self.bm25_enabled and self.bm25_retriever:
                bm25_results = self.bm25_retriever.search(
                    query=query, top_k=self.vector_top_k, collection_names=collections
                )

                # í•˜ì´ë¸Œë¦¬ë“œ ê²°í•©
                results = hybrid_search(
                    query=query,
                    vector_results=vector_results,
                    bm25_results=bm25_results,
                    vector_weight=self.bm25_vector_weight,
                    bm25_weight=self.bm25_weight,
                    top_k=self.vector_top_k,
                )
            else:
                results = vector_results

            return results

        except Exception as e:
            print(f"   [ERROR] ê²€ìƒ‰ ì¤‘ ì—ëŸ¬: {e}")  # â­
            import traceback

            traceback.print_exc()  # â­
            raise SearchError(str(e))

    def _detect_language(self, text: str) -> str:
        """
        ì§ˆë¬¸ ì–¸ì–´ ê°ì§€

        ê·œì¹™:cc
        1. í•œê¸€ í¬í•¨ â†’ í•œêµ­ì–´
        2. í•œê¸€ ì—†ê³  ì˜ì–´ í¬í•¨ â†’ ì˜ì–´
        3. ë‘˜ ë‹¤ ì—†ìŒ â†’ í•œêµ­ì–´ (ê¸°ë³¸ê°’)
        """
        # í•œê¸€ ë²”ìœ„: AC00-D7A3
        has_korean = any("\uac00" <= c <= "\ud7a3" for c in text)

        if has_korean:
            return "Korean"

        # ì˜ì–´ ì•ŒíŒŒë²³ ì²´í¬
        has_english = any(c.isascii() and c.isalpha() for c in text)

        return "English" if has_english else "Korean"

    @traceable(name="generate_answer", tags=["generation"])
    def _generate_answer(
        self, question: str, search_results: List[Dict], verbose: bool = True
    ) -> str:
        """LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""

        import time

        if verbose:
            print("\n[ë‹µë³€ ìƒì„± ì¤‘...]")

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (3b ëª¨ë¸: ê· í˜•ì¡íŒ ì»¨í…ìŠ¤íŠ¸)
        context_parts = []
        for i, result in enumerate(search_results[:3], 1):
            toc_section = result["metadata"].get("toc_section", "Unknown")
            text = result["text"][:300]  # 3b ëª¨ë¸ì€ 300ìë¡œ ì¶©ë¶„

            context_parts.append(
                f"[ë¬¸ì„œ {i}] {toc_section} (p.{result['metadata']['page']})\n{text}"
            )

        context = "\n\n".join(context_parts)
        context = context[:1000]  # 3b ëª¨ë¸ì€ 1000ìë¡œ ì¶©ë¶„ (ì†ë„ ìµœì í™”)

        # ì–¸ì–´ ê°ì§€
        question_lang = self._detect_language(question)

        # LangChain ìŠ¤íƒ€ì¼ ë²”ìš© í”„ë¡¬í”„íŠ¸ (1b ëª¨ë¸ ìµœì í™”)
        if question_lang == "Korean":
            prompt = f"""ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œë§Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

# ì°¸ê³  ë¬¸ì„œ
{context}

# ì§ˆë¬¸
{question}

# ë‹µë³€ ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜)
1. ìœ„ ì°¸ê³  ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ
3. ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³ ë§Œ ë‹µë³€
4. ì¶œì²˜ í˜ì´ì§€ ë²ˆí˜¸ ë°˜ë“œì‹œ í¬í•¨ (ì˜ˆ: "3í˜ì´ì§€ì— ë”°ë¥´ë©´...")
5. ê° ë¬¸ì¥ì€ 20ë‹¨ì–´ ì´ë‚´ë¡œ ì§§ê³  ëª…í™•í•˜ê²Œ ì‘ì„±
6. **ë§ì¶¤ë²•ê³¼ ë¬¸ë²•ì„ ì •í™•í•˜ê²Œ ì§€ì¼œì„œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€** (ì˜ì–´/ì¼ë³¸ì–´/ì¤‘êµ­ì–´ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€)

# ë‹µë³€ ì‘ì„±
ë‹µë³€:"""
        else:
            prompt = f"""You are a document-based Q&A expert. Extract information only from the given document and answer accurately.

# Reference Document
{context}

# Question
{question}

# Answer Rules (Must Follow)
1. Use only information explicitly stated in the document above
2. Never guess or add information not in the document
3. If answer not found, say only "I cannot find this information in the document"
4. Always include source page number (e.g., "According to page 3...")
5. Write 2-3 sentences, be concise
6. **Answer in English only**

# Answer
Answer:"""

        # ì‹œì‘ ì‹œê°„
        start_time = time.time()
        first_token_time = None
        full_response = ""
        chunk_count = 0
        max_time = 120  # 30ì´ˆ â†’ 120ì´ˆë¡œ ì¦ê°€ (CPU í™˜ê²½ ê³ ë ¤)

        # LLM í˜¸ì¶œ
        try:
            for chunk in self.llm.stream(prompt):
                # íƒ€ì„ì•„ì›ƒ ì²´í¬
                if time.time() - start_time > max_time:
                    raise TimeoutError(f"ë‹µë³€ ìƒì„± ì‹œê°„ ì´ˆê³¼ ({max_time}ì´ˆ)")

                if chunk_count == 0 and first_token_time is None:
                    first_token_time = time.time() - start_time
                    print(f"   [INFO] ì²« ì‘ë‹µ: {first_token_time:.1f}ì´ˆ\n")

                if chunk.content:
                    print(chunk.content, end="", flush=True)
                    full_response += chunk.content

                chunk_count += 1

            print()

        except KeyboardInterrupt:
            print("\n\n[ì¤‘ë‹¨] ì‚¬ìš©ìê°€ ìƒì„±ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            full_response += "\n\n[ë‹µë³€ ìƒì„± ì¤‘ë‹¨ë¨]"

        except TimeoutError as e:
            raise LLMGenerationError(self.llm.model, "ì‹œê°„ ì´ˆê³¼")

        except Exception as e:
            raise LLMGenerationError(self.llm.model, str(e))

        # í›„ì²˜ë¦¬
        full_response = self._clean_response(full_response)

        # ì¶œì²˜ ìë™ ì¶”ê°€
        if "ì°¸ê³ :" not in full_response and search_results:
            page = search_results[0]["metadata"]["page"]
            full_response += f"\n\nì°¸ê³ : p.{page}"

        # í†µê³„
        elapsed = time.time() - start_time
        generation_time = elapsed - (first_token_time or 0)

        print(f"   ğŸ“Š ì²« í† í°: {first_token_time:.1f}ì´ˆ")
        print(f"   ğŸ“Š ìƒì„±: {generation_time:.1f}ì´ˆ")
        print(f"   ğŸ“Š ì „ì²´: {elapsed:.1f}ì´ˆ")

        return full_response

    def _clean_response(self, text: str) -> str:
        """ì‘ë‹µ ì •ì œ"""
        import re

        # 1. í•œì ì œê±°
        text = re.sub(r"[ä¸€-é¾¯]", "", text)

        # 2. ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r"\n\s*\n\s*\n+", "\n\n", text)

        # 3. ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()

        return text

    def _extract_sources(self, search_results: List[Dict]) -> List[Dict]:
        sources = []
        for result in search_results:
            source = {
                "doc_name": result["metadata"]["doc_name"],
                "toc_section": result["metadata"].get("toc_section", "Unknown"),
                "page": result["metadata"]["page"],
                "score": result["score"],
            }
            # Rerank ì ìˆ˜ ì¶”ê°€ (ìˆìœ¼ë©´)
            if "rerank_score" in result:
                source["rerank_score"] = result["rerank_score"]

            sources.append(source)

        return sources

    def chat(self, doc_name: Optional[str] = None):
        """ëŒ€í™”í˜• ëª¨ë“œ"""
        print(f"\n{'='*70}")
        print("[CHAT MODE] ëŒ€í™”í˜• ì±—ë´‡ ëª¨ë“œ")
        print(f"{'='*70}")
        print(f"Reranking: {'Enabled' if self.rerank_enabled else 'Disabled'}")
        print("\nì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print(f"{'='*70}\n")

        while True:
            try:
                question = input("ì§ˆë¬¸: ").strip()

                if question.lower() in ["quit", "exit", "q"]:
                    print("\nì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
                    break

                if not question:
                    continue

                self.ask(question, doc_name=doc_name, verbose=True)

            except KeyboardInterrupt:
                print("\n\nì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
                break
            except Exception as e:
                print(f"\n[ERROR] {e}\n")


def create_chatbot() -> DocumentChatbot:
    """ì±—ë´‡ ìƒì„±"""
    return DocumentChatbot()


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        if sys.argv[1] == "chat":
            chatbot = create_chatbot()
            chatbot.chat()
        else:
            question = " ".join(sys.argv[1:])
            chatbot = create_chatbot()
            chatbot.ask(question)
    else:
        print("\nì‚¬ìš©ë²•:")
        print("  python chatbot.py chat")
        print('  python chatbot.py "ì§ˆë¬¸"')
