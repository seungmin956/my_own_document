# chatbot.py
"""
RAG ê¸°ë°˜ ë¬¸ì„œ ì±—ë´‡ (Reranking í†µí•©)

ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸:
1. Vector Search (ë¹ ë¥´ê²Œ 10ê°œ í›„ë³´)
2. Cross-Encoder Reranking (ì •í™•í•˜ê²Œ 5ê°œ ì„ íƒ)
3. LLM ë‹µë³€ ìƒì„±
"""

import os
from pathlib import Path
from typing import List, Dict, Optional
from dotenv import load_dotenv

# from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langsmith import traceable
from src.core.exceptions import (
    QdrantConnectionError,
    OllamaConnectionError,
    DocumentNotFoundError,
    SearchError,
    LLMGenerationError,
)
from src.services.retrieval.bm25_retriever import BM25Retriever, hybrid_search

# .env íŒŒì¼ ëª…ì‹œì  ë¡œë”©
env_path = Path(__file__).parent.parent.parent / ".env"
load_dotenv(env_path)

# í™˜ê²½ ê°ì§€
APP_ENV = os.getenv("APP_ENV", "production")
IS_PRODUCTION = APP_ENV == "production"

# ë””ë²„ê·¸: í™˜ê²½ ë³€ìˆ˜ í™•ì¸
if os.getenv("DEBUG", "false").lower() == "true":
    print(f"\n[DEBUG] í™˜ê²½ ë³€ìˆ˜:")
    print(f"  .env ê²½ë¡œ: {env_path}")
    print(f"  .env ì¡´ì¬: {env_path.exists()}")
    print(f"  APP_ENV: {APP_ENV}")
    print(f"  IS_PRODUCTION: {IS_PRODUCTION}")
    print()

if IS_PRODUCTION:
    from src.core.user_config import UserConfig as Config
else:
    from src.core import config as dev_config

    class Config:
        """ê°œë°œìš© ì„¤ì • ë˜í¼"""

        def __init__(self):
            self.qdrant_host = dev_config.QDRANT_HOST
            self.qdrant_port = dev_config.QDRANT_PORT
            self.qdrant_api_key = dev_config.QDRANT_API_KEY
            # Model Configuration (from .env)
            self.embedding_model = dev_config.EMBEDDING_MODEL
            self.llm_model = dev_config.LLM_MODEL
            self.llm_temperature = dev_config.LLM_TEMPERATURE
            self.top_k = 5
            self.score_threshold = 0.4
            # Reranking ì„¤ì •
            self.rerank_enabled = True
            self.rerank_model = dev_config.RERANK_MODEL
            self.rerank_max_candidates = 10
            self.rerank_top_k = 3
            # BM25 ì„¤ì • ì¶”ê°€
            self.bm25_enabled = dev_config.BM25_ENABLED
            self.bm25_vector_weight = dev_config.BM25_VECTOR_WEIGHT
            self.bm25_weight = dev_config.BM25_BM25_WEIGHT


from src.services.storage.qdrant_manager import QdrantManager
from src.services.retrieval.embedding_generator import EmbeddingGenerator
from src.services.retrieval.reranker import OptimizedReranker as Reranker

# LangSmith ì„¤ì •
LANGCHAIN_TRACING_V2 = os.getenv("LANGCHAIN_TRACING_V2", "false").lower() == "true"
LANGCHAIN_PROJECT = os.getenv("LANGCHAIN_PROJECT", "document-assistant-rag")


class DocumentChatbot:
    """
    RAG ê¸°ë°˜ ë¬¸ì„œ QA ì±—ë´‡ (Reranking í†µí•©)

    ê²€ìƒ‰ ì „ëµ:
    - Reranking OFF: Vector Searchë§Œ (ë¹ ë¦„)
    - Reranking ON: Vector + Cross-Encoder (ì •í™•)
    """

    def __init__(self, config: Optional[Config] = None):
        """
        Args:
            config: ì„¤ì • ê°ì²´ (Noneì´ë©´ ìë™ ìƒì„±)
        """
        # ì„¤ì • ë¡œë“œ
        if config is None:
            config = Config()

        self.config = config

        # 1. Qdrant ë§¤ë‹ˆì € (ì—°ê²° í™•ì¸ í¬í•¨)
        try:
            self.qdrant = QdrantManager(
                host=config.qdrant_host,
                port=config.qdrant_port,
                api_key=config.qdrant_api_key,
                embedding_dimension=1024,
                verbose=False,
            )
        except ConnectionError as e:
            # QdrantManager ë‚´ë¶€ì—ì„œ ì´ë¯¸ ConnectionError ë°œìƒ
            raise QdrantConnectionError(
                host=config.qdrant_host, port=config.qdrant_port
            )

        # 2. ì„ë² ë”© ìƒì„±ê¸° (ë¨¼ì € ìƒì„±)
        self.embedding_generator = EmbeddingGenerator(
            mode="local", model=config.embedding_model, verbose=False
        )

        # 3. Ollama ì—°ê²° í™•ì¸ (ìƒì„± í›„ í™•ì¸)
        try:
            self.embedding_generator.embed_query("test")
        except Exception as e:
            raise OllamaConnectionError(model=config.embedding_model)

        # 4. LLM ì´ˆê¸°í™”
        # vLLMì€ OpenAI APIì™€ í˜¸í™˜ë˜ë¯€ë¡œ ChatOpenAIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        self.llm = ChatOpenAI(
            model=config.llm_model,
            openai_api_key="EMPTY",  # vLLM ë¡œì»¬ ì„œë²„ëŠ” í‚¤ê°€ í•„ìš” ì—†ìŒ
            base_url="http://localhost:11434/v1",
            temperature=config.llm_temperature,
            max_tokens=4096,  # ë‹µë³€ ê¸¸ì´ ë„‰ë„‰í•˜ê²Œ
            streaming=True,
            # âœ… [ì¶”ê°€] ì´ ì¤„ì„ ë„£ìœ¼ë©´ ëª¨ë¸ì´ ì ˆëŒ€ ì ë“¤ì§€ ì•ŠìŠµë‹ˆë‹¤ (ë©”ëª¨ë¦¬ ê³„ì† ì ìœ )
            # model_kwargs={"keep_alive": -1},
        )

        # 5. Reranker (ì§€ì—° ë¡œë”©)
        self._reranker = None
        self.rerank_enabled = config.rerank_enabled

        if self.rerank_enabled:
            self._init_reranker(model_size=config.rerank_model, verbose=True)

        # ê²€ìƒ‰ ì„¤ì •
        self.score_threshold = config.score_threshold

        # Reranking ì„¤ì •
        if self.rerank_enabled:
            self.vector_top_k = config.rerank_max_candidates
            self.final_top_k = config.rerank_top_k
        else:
            self.vector_top_k = config.top_k
            self.final_top_k = config.top_k

        # 6. BM25 Retriever (ì„ íƒì )
        self.bm25_enabled = getattr(config, "bm25_enabled", False)
        if self.bm25_enabled:
            self.bm25_retriever = BM25Retriever(
                qdrant_manager=self.qdrant, verbose=False
            )
            self.bm25_vector_weight = getattr(config, "bm25_vector_weight", 0.7)
            self.bm25_weight = getattr(config, "bm25_weight", 0.3)
        else:
            self.bm25_retriever = None

        # ì´ˆê¸°í™” ë©”ì‹œì§€
        env_type = "í”„ë¡œë•ì…˜" if IS_PRODUCTION else "ê°œë°œ"

        print(f"\n{'='*70}")
        print(f"[INIT] Document Chatbot ì´ˆê¸°í™” ({env_type})")
        print(f"{'='*70}")

        if IS_PRODUCTION:
            print(f"  ì„¤ì •: ì‚¬ìš©ì ì„¤ì • (~/.document-assistant)")
        else:
            print(f"  ì„¤ì •: ê°œë°œ ì„¤ì • (.env)")

        print(f"  Qdrant: {config.qdrant_host}:{config.qdrant_port}")
        print(f"  Embedding: {config.embedding_model}")
        print(f"  LLM: {config.llm_model}")
        print(f"  ê²€ìƒ‰ ì „ëµ:")

        if self.bm25_enabled:
            print(f"    1. Hybrid Search (Vector + BM25)")
            print(
                f"       - Weights: Vector {self.bm25_vector_weight:.1f} / BM25 {self.bm25_weight:.1f}"
            )
            if self.rerank_enabled:
                print(f"    2. Reranking: {config.rerank_model} â†’ {self.final_top_k}ê°œ")
            print(f"       - Candidates: {self.vector_top_k}")
        elif self.rerank_enabled:
            print(
                f"    1. Vector Search: top_k={self.vector_top_k}, threshold={self.score_threshold}"
            )
            print(f"    2. Reranking: {config.rerank_model} â†’ top_k={self.final_top_k}")
        else:
            print(
                f"    Vector Searchë§Œ: top_k={self.vector_top_k}, threshold={self.score_threshold}"
            )

        if LANGCHAIN_TRACING_V2:
            print(f"  LangSmith: Enabled")

        print(f"{'='*70}\n")

    def _init_reranker(self, model_size: str = "auto", verbose: bool = False):
        """Reranker ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)"""
        self._reranker = Reranker(
            model_size=model_size, max_length=256, verbose=verbose
        )

    @traceable(name="rag_ask", tags=["rag", "chatbot"])
    def ask(
        self,
        question: str,
        doc_name: Optional[str] = None,
        chat_history: List[Dict[str, str]] = [],
        verbose: bool = True,
    ) -> Dict:
        try:
            if verbose:
                print(f"\n{'='*70}\n[QUESTION] {question}\n{'='*70}\n")

            # âœ… [ì¶”ê°€] ëŒ€í™” ë‚´ì—­ì´ ìˆìœ¼ë©´ ì§ˆë¬¸ì„ 'ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœ'ë¡œ ê³ ì³ì“°ê¸°
            search_query = question
            if chat_history:
                if verbose:
                    print("[0/4] ì§ˆë¬¸ ì¬êµ¬ì„± ì¤‘ (Contextualizing)...")
                new_question = self._rewrite_question(question, chat_history)
                if new_question != question:
                    print(f"   -> ì¬êµ¬ì„±ëœ ì§ˆë¬¸: {new_question}")
                    search_query = new_question

            # 1. ì„ë² ë”© (ì¬êµ¬ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰!)
            if verbose:
                print("[1/4] ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì¤‘...")
            query_embedding = self._embed_query(
                search_query
            )  # ğŸ‘ˆ question ëŒ€ì‹  search_query ì‚¬ìš©

            # 2. ê²€ìƒ‰ (ì¬êµ¬ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰!)
            if verbose:
                print(f"[2/4] ê²€ìƒ‰ ì¤‘ ({search_query})...")
            search_results = self._search(
                query_embedding, search_query, doc_name
            )  # ğŸ‘ˆ ì—¬ê¸°ë„ search_query

            if not search_results:
                if verbose:
                    print("\n[WARNING] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
                return {
                    "question": question,
                    "answer": "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "sources": [],
                    "error": None,
                }

            if verbose:
                print(f"   ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ì²­í¬")
                for i, result in enumerate(search_results[:3], 1):
                    toc_section = result["metadata"].get("toc_section", "Unknown")
                    print(
                        f"   [{i}] ì ìˆ˜: {result['score']:.3f} | "
                        f"{toc_section} (p.{result['metadata']['page']})"
                    )
                if len(search_results) > 3:
                    print(f"   ... ì™¸ {len(search_results) - 3}ê°œ")

            # 3. Reranking
            if self.rerank_enabled and self._reranker:
                if verbose:
                    print(
                        f"\n[3/4] Reranking ì¤‘ ({len(search_results)}ê°œ â†’ {self.final_top_k}ê°œ)..."
                    )

                try:
                    reranked_results = self._reranker.rerank(
                        query=question,
                        candidates=search_results,
                        top_k=self.final_top_k,
                    )

                    if verbose:
                        print(f"   [OK] Reranking ì™„ë£Œ: {len(reranked_results)}ê°œ ì„ íƒ")
                        for i, result in enumerate(reranked_results[:3], 1):
                            toc_section = result["metadata"].get(
                                "toc_section", "Unknown"
                            )
                            print(
                                f"   [{i}] Rerank: {result['rerank_score']:.3f} | "
                                f"Vector: {result['score']:.3f} | "
                                f"{toc_section}"
                            )

                    final_results = reranked_results

                except Exception as e:
                    if verbose:
                        print(f"\n[WARNING] Reranking ì‹¤íŒ¨: {e}")
                        print(f"   Vector Search ê²°ê³¼ ì‚¬ìš©\n")
                    final_results = search_results[: self.final_top_k]
            else:
                if verbose:
                    print(f"\n[3/4] Reranking ê±´ë„ˆëœ€")
                final_results = search_results[: self.final_top_k]

            # 4. ë‹µë³€ ìƒì„±
            if verbose:
                print(f"\n[4/4] ë‹µë³€ ìƒì„± ì¤‘ (LLM: {self.llm.model})...")

            answer = self._generate_answer(
                question, final_results, chat_history, verbose=verbose
            )

            if verbose:
                print(f"\n{'='*70}")
                print("[ANSWER]")
                print(f"{'='*70}")
                print(answer)
                print(f"\n{'='*70}")
                print("[SOURCES]")
                print(f"{'='*70}")

                sources = self._extract_sources(final_results)
                for i, source in enumerate(sources, 1):
                    toc_section = source.get("toc_section", "Unknown")
                    score_info = (
                        f"Rerank: {source.get('rerank_score', 0):.3f}"
                        if "rerank_score" in source
                        else f"Vector: {source['score']:.3f}"
                    )
                    print(
                        f"[{i}] {toc_section} (p.{source['page']}) | "
                        f"ë¬¸ì„œ: {source['doc_name']} | {score_info}"
                    )
                print(f"{'='*70}\n")

            return {
                "question": question,
                "answer": answer,
                "sources": self._extract_sources(final_results),
                "search_results": final_results,
                "error": None,
            }

        except (
            QdrantConnectionError,
            OllamaConnectionError,
            DocumentNotFoundError,
        ) as e:
            return {
                "question": question,
                "answer": None,
                "sources": [],
                "error": e.to_dict(),
            }

        except Exception as e:
            return {
                "question": question,
                "answer": None,
                "sources": [],
                "error": {
                    "error": "UnexpectedError",
                    "message": f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}",
                    "solution": "ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”",
                },
            }

    # âœ… [ìˆ˜ì •] LangChain ìŠ¤íƒ€ì¼ë¡œ í†µì¼ëœ ì§ˆë¬¸ ì¬êµ¬ì„± ë©”ì„œë“œ
    def _rewrite_question(
        self, question: str, chat_history: List[Dict[str, str]]
    ) -> str:
        """ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì„ 'ì™„ì „í•œ ë¬¸ì¥'ìœ¼ë¡œ ì¬êµ¬ì„±"""
        try:
            # ìµœê·¼ ëŒ€í™” 2ê°œë§Œ ì°¸ì¡° (ë„ˆë¬´ ê¸¸ë©´ í—·ê°ˆë ¤í•¨)
            history_text = ""
            for msg in chat_history[-2:]:
                role = "User" if msg["role"] == "user" else "Assistant"
                history_text += f"{role}: {msg['content']}\n"

            # 1. ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì •ì˜
            sys_msg = SystemMessage(
                content="""ë‹¹ì‹ ì€ ê²€ìƒ‰ ì—”ì§„ì„ ìœ„í•œ 'ì§ˆë¬¸ ìµœì í™” ë„êµ¬'ì…ë‹ˆë‹¤.
            ì‚¬ìš©ìì˜ 'í˜„ì¬ ì§ˆë¬¸'ì´ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½(ëŒ€ëª…ì‚¬ ë“±)ì— ì˜ì¡´í•œë‹¤ë©´, ì´ë¥¼ í¬í•¨í•˜ì—¬ 'ê²€ìƒ‰ ê°€ëŠ¥í•œ ì™„ë²½í•œ ë¬¸ì¥'ìœ¼ë¡œ ë‹¤ì‹œ ì“°ì„¸ìš”.
            ë§¥ë½ì´ í•„ìš” ì—†ë‹¤ë©´ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
            ì„¤ëª…ì´ë‚˜ ë¯¸ì‚¬ì—¬êµ¬ ì—†ì´ **ì˜¤ì§ ì¬êµ¬ì„±ëœ ì§ˆë¬¸ í•˜ë‚˜ë§Œ** ì¶œë ¥í•˜ì„¸ìš”."""
            )

            # 2. ì‚¬ìš©ì ë©”ì‹œì§€ ì •ì˜
            user_msg = HumanMessage(
                content=f"""
            # ëŒ€í™” ê¸°ë¡:
            {history_text}

            # í˜„ì¬ ì§ˆë¬¸:
            {question}

            # ì¬êµ¬ì„±ëœ ì§ˆë¬¸:"""
            )

            # 3. LLM í˜¸ì¶œ
            messages = [sys_msg, user_msg]
            response = self.llm.invoke(messages)

            return response.content.strip()

        except Exception as e:
            print(f"[WARN] ì§ˆë¬¸ ì¬êµ¬ì„± ì‹¤íŒ¨: {e}")
            return question

    @traceable(name="embed_query", tags=["embedding"])
    def _embed_query(self, question: str) -> List[float]:
        return self.embedding_generator.embed_query(question)

    @traceable(name="vector_search", tags=["retrieval"])
    def _search(
        self, query_vector: List[float], query: str, doc_name: Optional[str] = None
    ) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ ë˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        try:
            if doc_name:
                collections = self.qdrant.list_collections(doc_name)
                if not collections:
                    return []
            else:
                collections = None

            # ë²¡í„° ê²€ìƒ‰
            vector_results = self.qdrant.search(
                query_vector=query_vector,
                collection_names=collections,
                limit=self.vector_top_k,
                score_threshold=self.score_threshold,
            )
            # BM25 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            if self.bm25_enabled and self.bm25_retriever:
                bm25_results = self.bm25_retriever.search(
                    query=query, top_k=self.vector_top_k, collection_names=collections
                )

                # í•˜ì´ë¸Œë¦¬ë“œ ê²°í•©
                results = hybrid_search(
                    query=query,
                    vector_results=vector_results,
                    bm25_results=bm25_results,
                    vector_weight=self.bm25_vector_weight,
                    bm25_weight=self.bm25_weight,
                    top_k=self.vector_top_k,
                )
            else:
                results = vector_results

            return results

        except Exception as e:
            print(f"   [ERROR] ê²€ìƒ‰ ì¤‘ ì—ëŸ¬: {e}")
            import traceback

            traceback.print_exc()
            raise SearchError(str(e))

    def _detect_language(self, text: str) -> str:
        """
        ì§ˆë¬¸ ì–¸ì–´ ê°ì§€

        ê·œì¹™:
        1. í•œê¸€ í¬í•¨ â†’ í•œêµ­ì–´
        2. í•œê¸€ ì—†ê³  ì˜ì–´ í¬í•¨ â†’ ì˜ì–´
        3. ë‘˜ ë‹¤ ì—†ìŒ â†’ í•œêµ­ì–´ (ê¸°ë³¸ê°’)
        """
        # í•œê¸€ ë²”ìœ„: AC00-D7A3
        has_korean = any("\uac00" <= c <= "\ud7a3" for c in text)

        if has_korean:
            return "Korean"

        # ì˜ì–´ ì•ŒíŒŒë²³ ì²´í¬
        has_english = any(c.isascii() and c.isalpha() for c in text)

        return "English" if has_english else "Korean"

    @traceable(name="generate_answer", tags=["generation"])
    def _generate_answer(
        self,
        question: str,
        search_results: List[Dict],
        chat_history: List[Dict[str, str]] = [],
        verbose: bool = True,
    ) -> str:
        """LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""

        import time

        if verbose:
            print("\n[ë‹µë³€ ìƒì„± ì¤‘...]")

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (3b ëª¨ë¸: ê· í˜•ì¡íŒ ì»¨í…ìŠ¤íŠ¸)
        context_parts = []
        for i, result in enumerate(search_results[:3], 1):
            toc_section = result["metadata"].get("toc_section", "Unknown")
            text = result["text"][:300]  # 3b ëª¨ë¸ì€ 300ìë¡œ ì¶©ë¶„

            context_parts.append(
                f"[ë¬¸ì„œ {i}] {toc_section} (p.{result['metadata']['page']})\n{text}"
            )

        context = "\n\n".join(context_parts)
        context = context[:4000]

        # ì–¸ì–´ ê°ì§€
        question_lang = self._detect_language(question)

        # âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë‚´ìš© ì •ì˜ (ì§ˆë¬¸ì€ ì œì™¸)
        if question_lang == "Korean":
            sys_content = f"""ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œë§Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
            
            # ì ˆëŒ€ ì¤€ìˆ˜ ì‚¬í•­
            1. 'ä»¥ä¸‹', 'ä¾‹å¦‚' ê°™ì€ í•œìë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ëŒ€ì‹  'ë‹¤ìŒì€', 'ì˜ˆë¥¼ ë“¤ì–´' ê°™ì€ í•œêµ­ì–´ë¥¼ ì“°ì„¸ìš”.
            2. ë¬¸ë²•ê³¼ ë§ì¶¤ë²•ì„ ì™„ë²½í•œ í•œêµ­ì–´ë¡œ êµ¬ì‚¬í•˜ì„¸ìš”.
            3. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

            # ì°¸ê³  ë¬¸ì„œ
            {context}

            # ë‹µë³€ ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜)
            1. ìœ„ ì°¸ê³  ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©
            2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ
            3. ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³ ë§Œ ë‹µë³€
            4. ì¶œì²˜ í˜ì´ì§€ ë²ˆí˜¸ ë°˜ë“œì‹œ í¬í•¨ (ì˜ˆ: "3í˜ì´ì§€ì— ë”°ë¥´ë©´...")
            5. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ ë…¼ë¦¬ì ì´ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•  ê²ƒ
            6. í•„ìš”í•˜ë‹¤ë©´ ë²ˆí˜¸(Bullet points)ë¥¼ ë§¤ê²¨ ê°€ë…ì„±ì„ ë†’ì¼ ê²ƒ
            7. **ë§ì¶¤ë²•ê³¼ ë¬¸ë²•ì„ ì •í™•í•˜ê²Œ ì§€ì¼œì„œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€**"""
        else:
            sys_content = f"""You are a document-based Q&A expert. Extract information only from the given document and answer accurately.

            # Reference Document
            {context}

            # Answer Rules (Must Follow)
            1. Use only information explicitly stated in the document above
            2. Never guess or add information not in the document
            3. If answer not found, say only "I cannot find this information in the document"
            4. Always include source page number (e.g., "According to page 3...")
            5. Write 2-3 sentences, be concise
            6. **Answer in English only**"""

        # âœ… ë©”ì‹œì§€ ê°ì²´ ìƒì„± (ìë™ìœ¼ë¡œ Llama 3 íŠ¹ìˆ˜ í† í° ì ìš©ë¨)
        messages = [SystemMessage(content=sys_content)]

        # 1. íˆìŠ¤í† ë¦¬ ì£¼ì… (ìµœê·¼ 3ê°œ í„´ë§Œ ë°˜ì˜)
        # í”„ë¡ íŠ¸ì—ì„œ ë„˜ì–´ì˜¨ history: [{"role": "user", "content": "A"}, {"role": "assistant", "content": "B"}, ...]
        for msg in chat_history:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(AIMessage(content=msg["content"]))

        # 2. í˜„ì¬ ì§ˆë¬¸ ì£¼ì…
        messages.append(HumanMessage(content=question))

        # ì‹œì‘ ì‹œê°„
        start_time = time.time()
        first_token_time = None
        full_response = ""
        chunk_count = 0
        max_time = 120

        # LLM í˜¸ì¶œ (prompt ëŒ€ì‹  messages ì „ë‹¬)
        try:
            for chunk in self.llm.stream(messages):
                # íƒ€ì„ì•„ì›ƒ ì²´í¬
                if time.time() - start_time > max_time:
                    raise TimeoutError(f"ë‹µë³€ ìƒì„± ì‹œê°„ ì´ˆê³¼ ({max_time}ì´ˆ)")

                if chunk_count == 0 and first_token_time is None:
                    first_token_time = time.time() - start_time
                    print(f"   [INFO] ì²« ì‘ë‹µ: {first_token_time:.1f}ì´ˆ\n")

                if chunk.content:
                    print(chunk.content, end="", flush=True)
                    full_response += chunk.content

                chunk_count += 1

            print()

        except KeyboardInterrupt:
            print("\n\n[ì¤‘ë‹¨] ì‚¬ìš©ìê°€ ìƒì„±ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            full_response += "\n\n[ë‹µë³€ ìƒì„± ì¤‘ë‹¨ë¨]"

        except TimeoutError as e:
            raise LLMGenerationError(self.llm.model, "ì‹œê°„ ì´ˆê³¼")

        except Exception as e:
            raise LLMGenerationError(self.llm.model, str(e))

        # í›„ì²˜ë¦¬
        full_response = self._clean_response(full_response)

        # ì¶œì²˜ ìë™ ì¶”ê°€
        if "ì°¸ê³ :" not in full_response and search_results:
            page = search_results[0]["metadata"]["page"]
            full_response += f"\n\nì°¸ê³ : p.{page}"

        # í†µê³„
        elapsed = time.time() - start_time
        generation_time = elapsed - (first_token_time or 0)

        print(f"   ğŸ“Š ì²« í† í°: {first_token_time:.1f}ì´ˆ")
        print(f"   ğŸ“Š ìƒì„±: {generation_time:.1f}ì´ˆ")
        print(f"   ğŸ“Š ì „ì²´: {elapsed:.1f}ì´ˆ")

        return full_response

    def _clean_response(self, text: str) -> str:
        """ì‘ë‹µ ì •ì œ"""
        import re

        # 1. í•œì ì œê±° (ë²”ìœ„: 4E00-9FFF)
        text = re.sub(r"[\u4e00-\u9fff]", "", text)

        # 2. ê´„í˜¸ ì•ˆì— í•œìê°€ ë‚¨ì€ ê²½ìš° (ì˜ˆ: (ä»¥ä¸‹)) ì œê±°
        text = re.sub(r"\(\s*\)", "", text)

        # 3. ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r"\n\s*\n\s*\n+", "\n\n", text)

        # 4. ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()

        return text

    def _extract_sources(self, search_results: List[Dict]) -> List[Dict]:
        sources = []
        for result in search_results:
            source = {
                "doc_name": result["metadata"]["doc_name"],
                "toc_section": result["metadata"].get("toc_section", "Unknown"),
                "page": result["metadata"]["page"],
                "score": result["score"],
            }
            # Rerank ì ìˆ˜ ì¶”ê°€ (ìˆìœ¼ë©´)
            if "rerank_score" in result:
                source["rerank_score"] = result["rerank_score"]

            sources.append(source)

        return sources

    def chat(self, doc_name: Optional[str] = None):
        """ëŒ€í™”í˜• ëª¨ë“œ"""
        print(f"\n{'='*70}")
        print("[CHAT MODE] ëŒ€í™”í˜• ì±—ë´‡ ëª¨ë“œ")
        print(f"{'='*70}")
        print(f"Reranking: {'Enabled' if self.rerank_enabled else 'Disabled'}")
        print("\nì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print(f"{'='*70}\n")

        while True:
            try:
                question = input("ì§ˆë¬¸: ").strip()

                if question.lower() in ["quit", "exit", "q"]:
                    print("\nì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
                    break

                if not question:
                    continue

                self.ask(question, doc_name=doc_name, verbose=True)

            except KeyboardInterrupt:
                print("\n\nì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
                break
            except Exception as e:
                print(f"\n[ERROR] {e}\n")


def create_chatbot() -> DocumentChatbot:
    """ì±—ë´‡ ìƒì„±"""
    return DocumentChatbot()


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        if sys.argv[1] == "chat":
            chatbot = create_chatbot()
            chatbot.chat()
        else:
            question = " ".join(sys.argv[1:])
            chatbot = create_chatbot()
            chatbot.ask(question)
    else:
        print("\nì‚¬ìš©ë²•:")
        print("  python chatbot.py chat")
        print('  python chatbot.py "ì§ˆë¬¸"')
